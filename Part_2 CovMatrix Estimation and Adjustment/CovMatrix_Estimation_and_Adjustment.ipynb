{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "b2pS-XBBhke-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import *\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from xlrd import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from functools import reduce\n",
    "from tqdm.notebook import tqdm\n",
    "import utils as ut\n",
    "from scipy import linalg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JTE1icuThkfG"
   },
   "outputs": [],
   "source": [
    "## 读取数据\n",
    "# fqclose = pd.read_csv(\"stylefactor/fqclose.csv\", index_col=0)\n",
    "# ret5 = fqclose.pct_change(5, fill_method=None)\n",
    "\n",
    "# FactorRtn = pd.read_csv('factor_return5.csv', index_col=0)\n",
    "\n",
    "# SpecificRtn = ret5\n",
    "# MarketValue = pd.read_csv(\"stylefactor/floatmktcap.csv\", index_col=0)\n",
    "FactorRtn = pd.read_pickle('BarraFactorData/FactorReturn.pkl')\n",
    "SpecificRtn = pd.read_pickle('BarraFactorData/SpecificReturn.pkl').dropna(how='all',axis=1)\n",
    "MarketValue = pd.read_csv('stylefactor/floatmktcap.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0W0Sd4_KhkfH"
   },
   "outputs": [],
   "source": [
    "# # 只需要跑一次，保留因子暴露，每天暴露存一个文件\n",
    "# beta = pd.read_csv(\"BarraFactorData/Beta.csv\", index_col=0)\n",
    "# beta.index = beta.index.astype(str)\n",
    "# bp = pd.read_csv(\"BarraFactorData/BookToPrice.csv\", index_col=0)\n",
    "# bp.index = bp.index.astype(str)\n",
    "# ey = pd.read_csv(\"BarraFactorData/EarningsYield.csv\", index_col=0)\n",
    "# ey.index = ey.index.astype(str)\n",
    "# growth = pd.read_csv(\"BarraFactorData/Growth.csv\", index_col=0)\n",
    "# growth.index = growth.index.astype(str)\n",
    "# leverage = pd.read_csv(\"BarraFactorData/Leverage.csv\", index_col=0)\n",
    "# leverage.index = leverage.index.astype(str)\n",
    "# liq = pd.read_csv(\"BarraFactorData/Liquidity.csv\", index_col=0)\n",
    "# liq.index = liq.index.astype(str)\n",
    "# mom = pd.read_csv(\"BarraFactorData/Momentum.csv\", index_col=0)\n",
    "# mom.index = mom.index.astype(str)\n",
    "# nlsize = pd.read_csv(\"BarraFactorData/NonlinearSize.csv\", index_col=0)\n",
    "# nlsize.index = nlsize.index.astype(str)\n",
    "# resvol = pd.read_csv(\"BarraFactorData/ResidualVolatility.csv\", index_col=0)\n",
    "# resvol.index = resvol.index.astype(str)\n",
    "# size = pd.read_csv(\"BarraFactorData/Size.csv\", index_col=0)\n",
    "# size.index = size.index.astype(str)\n",
    "\n",
    "# for i in range(len(beta)):\n",
    "#     print(i, beta.iloc[i].name)\n",
    "#     indus_dummies = pd.get_dummies(indus.iloc[i])\n",
    "#     if 'Real Estate' not in indus_dummies:\n",
    "#         print(\"刚开始没有房地产这个行业，加一列0\")\n",
    "#         indus_dummies['Real Estate'] = 0\n",
    "\n",
    "#     date_df = pd.concat([beta.iloc[i], bp.iloc[i],ey.iloc[i], growth.iloc[i],\n",
    "#                          leverage.iloc[i], liq.iloc[i], mom.iloc[i],\n",
    "#                          nlsize.iloc[i], resvol.iloc[i], size.iloc[i],\n",
    "#                          # 注意这里有个坑，行业刚开始是31个后面变成了32个。需要调整\n",
    "#                          indus_dummies], axis=1)\n",
    "#     date_df.columns = ['Beta', 'BookToPrice', 'EarningsYield', 'Growth', 'Leverage',\n",
    "#                        'Liquidity', 'Momentum', 'NonlinearSize', 'ResidualVolatility', 'Size'] + indus_dummies.columns.tolist()\n",
    "#     date_df.insert(0, 'Market', 1)\n",
    "#     date_df = date_df[FactorRtn.columns]\n",
    "#     date_df.to_pickle(\"fac_data/fac_data\" + str(beta.iloc[i].name) + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_index(stock):\n",
    "    # 使用列表推导式和字符串处理将数据转换为新格式\n",
    "    new_stock = [f\"{x.split('.')[1]}.{x.split('.')[0][:2]}\" for x in stock]\n",
    "    # 打印转换后的列表\n",
    "    return new_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_pitch = pd.read_csv('eom_stocks_only_V9.csv',index_col = 0)\n",
    "date_pitch = pd.read_csv('last_trading_day_of_month.csv')\n",
    "date_pitch = list(date_pitch['Dates'])\n",
    "datetime_objects = [datetime.strptime(date, \"%Y-%m-%d %H:%M:%S%z\") for date in date_pitch]\n",
    "formatted_dates = [dt.strftime(\"%Y%m%d\") for dt in datetime_objects]\n",
    "stock_pitch.index = formatted_dates\n",
    "\n",
    "for i in range(len(stock_pitch)):\n",
    "    stock = transfer_index(stock_pitch.iloc[i,:])\n",
    "    stock_pitch.iloc[i,:] = stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_pitch = stock_pitch.reindex(index=SpecificRtn.index)\n",
    "stock_pitch.fillna(method='ffill', inplace=True)\n",
    "stock_pitch.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beta = date_pitch_fac(beta, index)\n",
    "# bp = date_pitch_fac(bp, index)\n",
    "# ey = date_pitch_fac(ey, index)\n",
    "# growth = date_pitch_fac(growth, index)\n",
    "# leverage = date_pitch_fac(leverage, index)\n",
    "# liq = date_pitch_fac(liq, index)\n",
    "# mom = date_pitch_fac(mom, index)\n",
    "# nlsize = date_pitch_fac(nlsize, index)\n",
    "# resvol = date_pitch_fac(resvol, index)\n",
    "# indus = date_pitch_fac(indus, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfxfyf6mhkfI"
   },
   "source": [
    "数据预处理 参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CY14W6OphkfM"
   },
   "outputs": [],
   "source": [
    "MarketValue.index = MarketValue.index.astype(str)\n",
    "FactorsValid = FactorRtn.columns.to_list()\n",
    "TradeDates = stock_pitch.index.to_list()\n",
    "\n",
    "FactorRtn = FactorRtn.loc[TradeDates,:] # (T, K)\n",
    "SpecificRtn = SpecificRtn.loc[TradeDates,:] # (T, N)\n",
    "MarketValue = MarketValue.loc[TradeDates,:] # (T, N)\n",
    "\n",
    "T = FactorRtn.shape[0] # 历史样本数\n",
    "K = FactorRtn.shape[1] # 风险因子数\n",
    "\n",
    "## 设定计算参数\n",
    "K_inds = 32\n",
    "length = 90\n",
    "lag_F_Corr = 2\n",
    "tau_F_Corr = 480\n",
    "lag_F_Vol = 5\n",
    "# 周度调整为20，月度为81和42\n",
    "tau_F_Vol = 81\n",
    "tau_F_VRA = 42\n",
    "\n",
    "q_S_BS = 0.1 # 特异收益率-贝叶斯收缩 参数\n",
    "lag_S_Vol = 5\n",
    "# 周度调整为20，月度为80和40\n",
    "tau_S_Vol = 80\n",
    "tau_S_VRA = 40\n",
    "\n",
    "FR = FactorRtn.values\n",
    "\n",
    "BF_F_all=[]  \n",
    "BF_S_all=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmkCYGMXhkfN"
   },
   "source": [
    "需要用到的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "WTsz-6LihkfO"
   },
   "outputs": [],
   "source": [
    "def uniformize(x):\n",
    "    return x / np.nansum(x)\n",
    "\n",
    "def normalize(x, weight, std_weight=None):\n",
    "    mean = np.nansum(x * uniformize(weight))\n",
    "    if std_weight is not None:\n",
    "        std = np.nanmean((x-mean)**2 * std_weight)\n",
    "    else:\n",
    "        std = np.nanstd(x)\n",
    "    return (x - mean) / std\n",
    "\n",
    "def winsorize(x):\n",
    "    '''确保x是经过标准化的数据'''\n",
    "    s_pos = max(0,min(1,0.5/(max(x)-3)))\n",
    "    s_neg = max(0,min(1,0.5/(-min(x)-3)))\n",
    "    x[x > 3] = 3*(1 - s_pos) + x[x > 3] * s_pos\n",
    "    x[x < -3] = -3*(1 - s_neg) + x[x < -3] * s_neg\n",
    "    return x\n",
    "\n",
    "def winsorize_percentile(x, per_upper=1, per_lower=0):\n",
    "    upper = np.percentile(x, per_upper*100)\n",
    "    lower = np.percentile(x, per_lower*100)\n",
    "    x[x < lower] = lower\n",
    "    x[x > upper] = upper\n",
    "    return x\n",
    "\n",
    "def orthogonalize(raw, ref, weight=None):\n",
    "    if weight is not None:\n",
    "        ortho = raw - (raw * ref * weight).sum() / (ref**2 * weight).sum() * ref\n",
    "    else:\n",
    "        ortho = raw - np.nansum(raw * ref) / np.nansum(ref**2) * ref\n",
    "    return ortho\n",
    "\n",
    "def fillna_with_regression(x, mask, ref):\n",
    "    x_valid = x[mask,None]\n",
    "    ref = np.vstack([np.ones_like(ref),ref]).T\n",
    "    ref_valid = ref[mask,:]\n",
    "    beta = np.linalg.inv(ref_valid.T @ ref_valid) @ ref_valid.T @ x_valid\n",
    "    x[~mask] = (ref[~mask,:] @ beta).ravel()\n",
    "    return x\n",
    "\n",
    "\n",
    "def data_clean(x, w, fillna_ref=None):\n",
    "    ValidMask = ~(np.isnan(x) | np.isinf(x))\n",
    "    x_valid = x[ValidMask]\n",
    "    w_valid = uniformize(w[ValidMask])\n",
    "    x_valid = normalize(winsorize(normalize(x_valid, w_valid)),w_valid)\n",
    "    x[ValidMask] = x_valid\n",
    "    if fillna_ref is not None:\n",
    "        x = normalize(fillna_with_regression(x, ValidMask, fillna_ref), w)\n",
    "    return x\n",
    "\n",
    "def get_EWMA(length=252, tau=90, norm=1):\n",
    "    lambd = 0.5**(1./tau)\n",
    "    w = np.array([lambd**n for n in range(length)][::-1])\n",
    "    if norm:\n",
    "        return uniformize(w)\n",
    "    else:\n",
    "        return w\n",
    "\n",
    "def v_parabolicfit_scaling(v_raw, amp=1.6, k_start_fitting=15):\n",
    "    '''\n",
    "    1、Put v_raw into parabolic fit, get v_p\n",
    "    2、Scale the fitted value in proportion to their deivation from 1, get v_s\n",
    "    '''\n",
    "    K = len(v_raw)\n",
    "    y = v_raw[k_start_fitting:]\n",
    "    x = np.array(range(k_start_fitting,K))\n",
    "    polyfunc = np.poly1d(np.polyfit(x, y, 2))\n",
    "    v_p = np.array([polyfunc(xi) for xi in range(k_start_fitting)] + list(y))\n",
    "    v_s = amp * (v_p - 1) + 1\n",
    "    return v_s\n",
    "\n",
    "def getNeweyWestCov(rtn_data, q, tau, length, n_this, is_NW=1):\n",
    "    '''\n",
    "    Input:\n",
    "        rtn_data            The factor / specific return\n",
    "        q                   Num of periods used to calculate Newey-West Adjusted Covariance\n",
    "        tau                 The length of Halflife Decay\n",
    "        length              How many frames are used to calculate the covariance matrix\n",
    "                            The length must be greater than K (the No. of factors)\n",
    "        n_this              'n_this-length' to 'n_this' frames are used to get the covariance\n",
    "        n_forward           The period ahead to calculate bias B; standard deviation of r_ahead / risk_predicted\n",
    "        is_NW               whether use Newey-West Method\n",
    "\n",
    "     n_this-length           n_this                 n_this+n_forward\n",
    "          |---------------------|-------------------------|\n",
    "                   F                       Bias\n",
    "\n",
    "    Output:\n",
    "        F           if is_NW==True: Newey-West Seriel Correlation Adjustment Covariance;\n",
    "                    elif: Simple Covariance.\n",
    "    '''\n",
    "    q=2\n",
    "    tau=90\n",
    "    if n_this < length:\n",
    "        print('ERROR: n_this should not be less than length')\n",
    "        return\n",
    "    elif n_this > rtn_data.shape[0]:\n",
    "        print('ERROR: n_this + n_forward should not be greater than No. of total frames')\n",
    "        return\n",
    "    f_raw = rtn_data[n_this-length:n_this,:]\n",
    "\n",
    "    # calculate Newey-West covariance\n",
    "    if is_NW:\n",
    "        # 半衰期加权所用权重，wighted average of factors\n",
    "        w = get_EWMA(length, tau, 1)\n",
    "        ewma = np.dot(w[::-1], f_raw)\n",
    "        f = (f_raw - ewma).T\n",
    "        Delta_0 = f @ np.diag(w) @ f.T\n",
    "        # Calculate the cov matrix\n",
    "        F = Delta_0\n",
    "        for d in range(1, q+1):\n",
    "            # 计算滞后d期的协方差（非对称）\n",
    "            Delta_i = f[:,:-d] @ np.diag(w[d:]) @ f[:,d:].T / np.sum(w[d:])\n",
    "            # 乘以Bartlett权重系数\n",
    "            F += ( (1 - d/(q+1.)) * (Delta_i + Delta_i.T) )\n",
    "    else:\n",
    "        F = np.cov(f_raw.T)\n",
    "    # 日频协方差月频化,21 for monthly\n",
    "    F = 21. * F\n",
    "    # decomp of NW covariance, Prepare for Eigenfactor Risk Adjustment\n",
    "    D_diag, U = linalg.eigh(F) # D  in fact the diagonal element of matrix D, a.k.a EigenVar\n",
    "    # r = (data.iloc[n_start:n_start+n_forward,:]+1).cumprod().iloc[-1,:]-1\n",
    "    # R_eigen = np.dot(U.T,r)\n",
    "\n",
    "    # check if U and D_diag is the right eigh-decomposition of F\n",
    "    if not np.allclose(F,  U @ np.diag(D_diag) @ U.T ):\n",
    "        print('ERROR in eigh')\n",
    "        return\n",
    "\n",
    "    return F, U, D_diag\n",
    "\n",
    "def getNeweyWestVol(rtn_data, q, tau, length, n_this, is_NW=1):\n",
    "    '''\n",
    "    单变量版本的Newey-West调整\n",
    "    rtn_data 此处为pd.Seires格式 便于使用apply方法计算。\n",
    "    '''\n",
    "    f_raw = rtn_data.iloc[n_this-length:n_this]\n",
    "    if f_raw.dropna().size < length//4:\n",
    "        ### 数据过少，直接给出波动率为0\n",
    "        Vol = 0\n",
    "        return Vol\n",
    "    f_raw = f_raw.dropna().values\n",
    "    length = f_raw.size\n",
    "    if is_NW:\n",
    "        # 半衰期加权所用权重，wighted average of factors\n",
    "        w = get_EWMA(length, tau, 1)\n",
    "        ewma = np.dot(w[::-1], f_raw)\n",
    "        f = f_raw - ewma\n",
    "        Var_0 = (f**2 * w).sum()\n",
    "        # Calculate the cov matrix\n",
    "        Var = Var_0\n",
    "        for d in range(1, q+1):\n",
    "            # 计算滞后d期的协方差（非对称）\n",
    "            Var_i = (f[:-d] * f[d:] * w[d:]).sum() / np.sum(w[d:])\n",
    "            # 乘以Bartlett权重系数\n",
    "            Var += ( (1 - d/(q+1.)) * (Var_i * 2) )\n",
    "        Vol = np.sqrt(Var)\n",
    "    else:\n",
    "        Vol = np.std(f_raw)\n",
    "    return Vol\n",
    "\n",
    "def getEigenfactorRiskAdjustCov(F_0, U_0, D_diag, tau, length, M=100, is_NW=1): # M should be 10^4 as in report\n",
    "    '''\n",
    "    Input:\n",
    "        F_0         Un-adjusted factor covariance matrix\n",
    "        U_0         KxK rotation matrix whose columns are given by the eigenvectors of F_0.\n",
    "                    U_0[j,k] gives the weight of pure factor j in eigenfactor k .\n",
    "        D_diag      the diagonal element of D, which is the predicted variances of the eigenfactors\n",
    "        M           the total number of simulations\n",
    "        is_NW       whether use Newey-West Method\n",
    "    Output:\n",
    "        F_true      Predicted Factor Covariance After Eigenfactor Risk Adjustment\n",
    "    '''\n",
    "    K = F_0.shape[0]\n",
    "    simulated_volatility_biases = np.zeros((M, K))\n",
    "    for i in range(M):\n",
    "        # if i%50==0:print(i)\n",
    "        b_m = np.array([np.random.normal(0, std, length) for std in np.sqrt(D_diag)]) # generate simulated factor returns\n",
    "        f_m = (U_0 @ b_m).T\n",
    "        F_m, U_m, D_diag_m = getNeweyWestCov(f_m, q=2, tau=tau, length=length, n_this=length, is_NW=is_NW)\n",
    "        D_diag_m_true = np.diagonal(U_m.T @ F_0 @ (U_m))\n",
    "        simulated_volatility_biases[i,:] = D_diag_m_true / D_diag_m\n",
    "    # Simulated Volatility Biases\n",
    "    v_raw = np.sqrt(simulated_volatility_biases.mean(axis=0))\n",
    "    # After Parabolic fit & Scaling\n",
    "    v_s = v_parabolicfit_scaling(v_raw, amp=1.6, k_start_fitting=16)\n",
    "    D_true = np.diag(v_s**2 * D_diag)\n",
    "    F_true = U_0 @ D_true @ U_0.T\n",
    "    return F_true\n",
    "    \n",
    "\n",
    "def getVolatilityRegimeAdjustment(rtn_data, Vol, tau, length, n_this, BF_all):\n",
    "    '''\n",
    "    Input:\n",
    "        rtn_data        The factor / specific return\n",
    "        Vol             The volatility of factor / specific return estimated on day n_this\n",
    "        tau             Halflife parameter of VRA\n",
    "        length          The length of factor_data used to calculate the volatility multiplier\n",
    "        n_this\n",
    "        BF_all          Store each day's Factor Cross-Sectional Bias statistic B^F_t\n",
    "    Output:\n",
    "        Vol_VRA         the Volatility Regime Adjustment forecast of Factor Volatility\n",
    "        BF_all          the same\n",
    "    '''\n",
    "    tau=42\n",
    "    w = get_EWMA(length, tau, 1)\n",
    "    #Vol=np.diag(Vol) # to solve the dimension bug on b\n",
    "    isValidVol = (Vol != 0)\n",
    "    ## b is the cross-section standardized factor return\n",
    "    ## BF is the bias statistic for realized standard deviation of b\n",
    "    #b = rtn_data[n_this,:][isValidVol] / Vol[isValidVol]\n",
    "    b = rtn_data[n_this,:][isValidVol]/ Vol[isValidVol]\n",
    "    BF = np.sqrt(np.mean(b @ b))\n",
    "    BF_all.append(BF)\n",
    "    if n_this > (length+243): # deal with NaN in too early data\n",
    "        # lambda_F is volatility multiplier \n",
    "        lambda_F = np.sqrt( np.power(BF_all[-length:],2) @ w )\n",
    "        Vol_VRA = lambda_F**2 * Vol # lambda_F可以直接平方吗？a number?\n",
    "    else:\n",
    "        Vol_VRA = np.array([np.NaN] * len(Vol))\n",
    "    \n",
    "    return Vol_VRA, BF_all\n",
    "\n",
    "def getSVDAdjustment(cov):\n",
    "    if np.isnan(Cov_F).all():\n",
    "        return None\n",
    "    else:\n",
    "        U,s,V = linalg.svd(cov)\n",
    "        s[s <= 0] = min(s[s > 0]) / 10\n",
    "        cov_adj = U @ np.diag(s) @ V\n",
    "        return cov_adj\n",
    "\n",
    "def getBlendWeight(Rtn_seires, length, n_this):\n",
    "    f_raw = Rtn_seires.iloc[n_this-length:n_this].values\n",
    "    sigma_u = (np.percentile(f_raw, 75) - np.percentile(f_raw, 25)) / 1.35\n",
    "    f_raw[f_raw < -10*sigma_u] = -10*sigma_u\n",
    "    f_raw[f_raw > 10*sigma_u] = 10*sigma_u\n",
    "    sigma_eq = np.std(f_raw)\n",
    "    Z_u = np.abs((sigma_eq-sigma_u)/sigma_u)\n",
    "    gamma = min(1, max(0, np.exp(1-Z_u)))\n",
    "    return gamma\n",
    "\n",
    "def getBlendedSpecificVol(FactorExposure, Gamma, Vol_TS):\n",
    "    mask_gamma = (Gamma == 1)\n",
    "    y = np.log(Vol_TS[mask_gamma])\n",
    "    x = FactorExposure[mask_gamma,:]\n",
    "    b = linalg.inv(x.T @ x) @ x.T @ y\n",
    "    Vol_STR_0 = np.exp(FactorExposure @ b)\n",
    "    Vol_hat = Gamma * Vol_TS + (1 - Gamma) * np.mean(Vol_TS / Vol_STR_0) * Vol_STR_0\n",
    "    return Vol_hat\n",
    "\n",
    "def getBayesianShrinkageResult(Vol, MktCap, n_decile, q):\n",
    "    '''\n",
    "    Input:\n",
    "        Vol             Volaitlity of specific return of each stock\n",
    "        MktCap          Market Value of each stock\n",
    "        n_decile        number of group the stocks will be split into by market value\n",
    "        q               Shrinkage Parameter\n",
    "    Output:\n",
    "        Vol_BS          Volatility after Bayesian Shrinkage\n",
    "    '''\n",
    "    isValidVol = (Vol != 0)\n",
    "    Vol_BS = np.zeros(Vol.size)\n",
    "    Vol = Vol[isValidVol]\n",
    "    MktCap = MktCap[isValidVol]\n",
    "    IdxRankedGrouped = np.array_split(np.argsort(MktCap), n_decile) # 从小到大排列的市值对应序号\n",
    "    # Get the Shrinkage Target of each decile group\n",
    "    VolTarget_G = np.array([np.average(Vol[idxs], weights=uniformize(MktCap[idxs])) for idxs in IdxRankedGrouped])\n",
    "    # Get the standard deviation of specific risk forecasts of each decile group\n",
    "    VolDelta_G = np.array([((Vol[idxs] - vt)**2).sum() / len(idxs) for vt, idxs in zip(VolTarget_G, IdxRankedGrouped)])\n",
    "    # Broadcast group values to all valid vol\n",
    "    VolTarget = np.zeros(Vol.size)\n",
    "    VolDelta = np.zeros(Vol.size)\n",
    "    for i_group, idxs in enumerate(IdxRankedGrouped):\n",
    "        VolTarget[idxs] = VolTarget_G[i_group]\n",
    "        VolDelta[idxs] = VolDelta_G[i_group]\n",
    "    # v is the shrinkage intensity\n",
    "    v = 1 / (VolDelta / q * np.abs(Vol - VolTarget) + 1)\n",
    "    Vol_BS_Valid = v * VolTarget + (1-v) * Vol\n",
    "    Vol_BS[isValidVol] = Vol_BS_Valid\n",
    "    return Vol_BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2c328813124510abed8032849cf564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 计算因子收益率的相关系数矩阵\u001b[39;00m\n\u001b[0;32m     22\u001b[0m Cov_F_NW_toCorr, U_toCorr, D_diag_toCorr \u001b[38;5;241m=\u001b[39m getNeweyWestCov(FR, q\u001b[38;5;241m=\u001b[39mlag_F_Corr, tau\u001b[38;5;241m=\u001b[39mtau_F_Corr, length\u001b[38;5;241m=\u001b[39mlength, n_this\u001b[38;5;241m=\u001b[39midx_date, is_NW\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m Cov_F \u001b[38;5;241m=\u001b[39m \u001b[43mgetEigenfactorRiskAdjustCov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCov_F_NW_toCorr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mU_toCorr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_diag_toCorr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau_F_Corr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_NW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m Cov_F_tilde \u001b[38;5;241m=\u001b[39m getSVDAdjustment(Cov_F) \u001b[38;5;66;03m# 正定化\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 计算特异收益率的波动率\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 182\u001b[0m, in \u001b[0;36mgetEigenfactorRiskAdjustCov\u001b[1;34m(F_0, U_0, D_diag, tau, length, M, is_NW)\u001b[0m\n\u001b[0;32m    180\u001b[0m b_m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0\u001b[39m, std, length) \u001b[38;5;28;01mfor\u001b[39;00m std \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqrt(D_diag)]) \u001b[38;5;66;03m# generate simulated factor returns\u001b[39;00m\n\u001b[0;32m    181\u001b[0m f_m \u001b[38;5;241m=\u001b[39m (U_0 \u001b[38;5;241m@\u001b[39m b_m)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m--> 182\u001b[0m F_m, U_m, D_diag_m \u001b[38;5;241m=\u001b[39m \u001b[43mgetNeweyWestCov\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_m\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_this\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_NW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_NW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m D_diag_m_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiagonal(U_m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m F_0 \u001b[38;5;241m@\u001b[39m (U_m))\n\u001b[0;32m    184\u001b[0m simulated_volatility_biases[i,:] \u001b[38;5;241m=\u001b[39m D_diag_m_true \u001b[38;5;241m/\u001b[39m D_diag_m\n",
      "Cell \u001b[1;32mIn[26], line 74\u001b[0m, in \u001b[0;36mgetNeweyWestCov\u001b[1;34m(rtn_data, q, tau, length, n_this, is_NW)\u001b[0m\n\u001b[0;32m     71\u001b[0m     v_s \u001b[38;5;241m=\u001b[39m amp \u001b[38;5;241m*\u001b[39m (v_p \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m v_s\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNeweyWestCov\u001b[39m(rtn_data, q, tau, length, n_this, is_NW\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m    Input:\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m        rtn_data            The factor / specific return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m                    elif: Simple Covariance.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     q\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Vol_S_df = pd.read_pickle('FactorCov&SpecificVol/SpecificVol.pkl')\n",
    "for idx_date in tqdm(range(length,T)):\n",
    "    date = TradeDates[idx_date]\n",
    "    StocksValid = stock_pitch.iloc[idx_date,:].values\n",
    "    \n",
    "    # print(idx_date, date)\n",
    "    FactorExposure = pd.read_pickle(\"fac_data/fac_data\" + str(date) + \".pkl\")\n",
    "    index_FE = FactorExposure.index\n",
    "    columns_FE = FactorExposure.columns\n",
    "    FactorExposure = np.nan_to_num(FactorExposure, nan=0)\n",
    "    FactorExposure = pd.DataFrame(FactorExposure)\n",
    "    FactorExposure.columns = columns_FE\n",
    "    FactorExposure.index = index_FE\n",
    "    StocksValid = [i for i in StocksValid if i in FactorExposure.index]\n",
    "    \n",
    "    FactorExposure = FactorExposure.loc[StocksValid,FactorsValid[:-K_inds]].values\n",
    "    SR = SpecificRtn.loc[:,StocksValid].values\n",
    "    thisMV = MarketValue.loc[date,StocksValid].values\n",
    "\n",
    "\n",
    "    # 计算因子收益率的相关系数矩阵\n",
    "    Cov_F_NW_toCorr, U_toCorr, D_diag_toCorr = getNeweyWestCov(FR, q=lag_F_Corr, tau=tau_F_Corr, length=length, n_this=idx_date, is_NW=1)\n",
    "    Cov_F = getEigenfactorRiskAdjustCov(Cov_F_NW_toCorr, U_toCorr, D_diag_toCorr, tau=tau_F_Corr, length=length, M=500, is_NW=0)\n",
    "    Cov_F_tilde = getSVDAdjustment(Cov_F) # 正定化\n",
    "    \n",
    "    # 计算特异收益率的波动率\n",
    "    SpecificRtnValid = SpecificRtn.loc[:,StocksValid]\n",
    "    Vol_S_NW = SpecificRtnValid.apply(lambda x:getNeweyWestVol(x, q=lag_S_Vol, tau=tau_S_Vol, length=length, n_this=idx_date, is_NW=1),axis=0).values\n",
    "    Gamma = SpecificRtnValid.apply(lambda x:getBlendWeight(x, length=length, n_this=idx_date), axis=0).values\n",
    "    Vol_S_Blend = getBlendedSpecificVol(FactorExposure, Gamma, Vol_S_NW)\n",
    "    Vol_S_BS = getBayesianShrinkageResult(Vol_S_Blend, thisMV, n_decile=10, q=q_S_BS)\n",
    "    if Vol_S_BS.shape[0] != 40:\n",
    "        Vol_S_BS = np.append(Vol_S_BS, 0)\n",
    "    # 记录数据\n",
    "    if Cov_F_tilde is not None:\n",
    "        Vol_S_df.loc[date, :] = Vol_S_BS\n",
    "        pd.DataFrame(Cov_F_tilde, index=FactorsValid, columns=FactorsValid).to_pickle(f'FactorCov&SpecificVol/FactorCov_{date}.pkl')\n",
    "        #\n",
    "\n",
    "Vol_S_df = Vol_S_df.dropna(how='all',axis=0)\n",
    "Vol_S_df.to_pickle('FactorCov&SpecificVol/SpecificVol.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWKsSLsVhkfY"
   },
   "source": [
    "现在F,Δ都有了。进一步加工搞成股票协方差矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta = pd.read_pickle(\"FactorCov&SpecificVol/SpecificVol.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "Delta = Delta.iloc[:,:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_pitch = stock_pitch.iloc[:,:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2432, 11)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_pitch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_stock_pitch_date = []\n",
    "\n",
    "for datei in stock_pitch_date:\n",
    "    if datei in Delta.index:\n",
    "        filtered_stock_pitch_date.append(datei)\n",
    "\n",
    "# Replace stock_pitch_date with the filtered list\n",
    "stock_pitch_date = filtered_stock_pitch_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20130731\n",
      "20130830\n",
      "20130930\n",
      "20131031\n",
      "20131129\n",
      "20131231\n",
      "20140130\n",
      "20140228\n",
      "20140331\n",
      "20140430\n",
      "20140530\n",
      "20140630\n",
      "20140731\n",
      "20140829\n",
      "20140930\n",
      "20141031\n",
      "20141128\n",
      "20141231\n",
      "20150130\n",
      "20150227\n",
      "20150331\n",
      "20150430\n",
      "20150529\n",
      "20150630\n",
      "20150731\n",
      "20150831\n",
      "20150930\n",
      "20151030\n",
      "20151130\n",
      "20151231\n",
      "20160129\n",
      "20160229\n",
      "20160331\n",
      "20160429\n",
      "20160531\n",
      "20160630\n",
      "20160729\n",
      "20160831\n",
      "20160930\n",
      "20161031\n",
      "20161130\n",
      "20161230\n",
      "20170126\n",
      "20170228\n",
      "20170331\n",
      "20170428\n",
      "20170531\n",
      "20170630\n",
      "20170731\n",
      "20170831\n",
      "20170929\n",
      "20171031\n",
      "20171130\n",
      "20171229\n",
      "20180131\n",
      "20180228\n",
      "20180330\n",
      "20180427\n",
      "20180531\n",
      "20180629\n",
      "20180731\n",
      "20180831\n",
      "20180928\n",
      "20181031\n",
      "20181130\n",
      "20181228\n",
      "20190131\n",
      "20190228\n",
      "20190329\n",
      "20190430\n",
      "20190531\n",
      "20190628\n",
      "20190731\n",
      "20190830\n",
      "20190930\n",
      "20191031\n",
      "20191129\n",
      "20191231\n",
      "20200123\n",
      "20200228\n",
      "20200331\n",
      "20200430\n",
      "20200529\n",
      "20200630\n",
      "20200731\n",
      "20200831\n",
      "20200930\n",
      "20201030\n",
      "20201130\n",
      "20201231\n",
      "20210129\n",
      "20210226\n",
      "20210331\n",
      "20210430\n",
      "20210531\n",
      "20210630\n",
      "20210730\n",
      "20210831\n",
      "20210930\n",
      "20211029\n",
      "20211130\n",
      "20211231\n",
      "20220128\n",
      "20220228\n",
      "20220331\n",
      "20220429\n",
      "20220531\n",
      "20220630\n",
      "20220729\n",
      "20220831\n",
      "20220930\n",
      "20221031\n",
      "20221130\n",
      "20221230\n",
      "20230131\n",
      "20230228\n"
     ]
    }
   ],
   "source": [
    "for datei in stock_pitch_date:\n",
    "    print(datei)\n",
    "    F = pd.read_pickle(\"FactorCov&SpecificVol/FactorCov_\" + str(datei) + \".pkl\")\n",
    "    X = pd.read_pickle(\"fac_data/fac_data\" + str(datei) + \".pkl\")\n",
    "    X = X.loc[:,F.index]\n",
    "    common_stocks = [i for i in stock_pitch.loc[datei] if i in X.index]\n",
    "    \n",
    "    # 股票保持一致\n",
    "    X = X.loc[common_stocks]\n",
    "    # Delta_i = Delta[common_stocks]\n",
    "\n",
    "\n",
    "    # delta取指定这一天的, 然后变成一个对角矩阵\n",
    "    Delta_i = Delta.loc[str(datei)]\n",
    "    Delta_diag = pd.DataFrame(np.diag(Delta_i))\n",
    "    Delta_diag.columns = X.index \n",
    "    Delta_diag.index = X.index \n",
    "\n",
    "    CovStocks = X @ F @ X.T + Delta_diag\n",
    "    df = pd.DataFrame(CovStocks)\n",
    "    \n",
    "    filename = \"StockCov_\" + str(datei) + \".csv\"\n",
    "    # 创建名为StockCov的文件夹\n",
    "    folder_name = 'StockCov'\n",
    "    path = os.path.join(os.getcwd(), folder_name)\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"文件夹 '{folder_name}' 创建成功\")\n",
    "\n",
    "    # 将CSV文件保存到StockCov文件夹中\n",
    "    csv_file_path = os.path.join(path, filename)  # CSV文件路径\n",
    "    df.to_csv(csv_file_path, index=False)  # 将DataFrame保存为CSV文件\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "原始单元格格式",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
