{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "b2pS-XBBhke-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import *\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from xlrd import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from functools import reduce\n",
    "from tqdm.notebook import tqdm\n",
    "import utils as ut\n",
    "from scipy import linalg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "JTE1icuThkfG"
   },
   "outputs": [],
   "source": [
    "## 读取数据\n",
    "# fqclose = pd.read_csv(\"stylefactor/fqclose.csv\", index_col=0)\n",
    "# ret5 = fqclose.pct_change(5, fill_method=None)\n",
    "\n",
    "# FactorRtn = pd.read_csv('factor_return5.csv', index_col=0)\n",
    "\n",
    "# SpecificRtn = ret5\n",
    "# MarketValue = pd.read_csv(\"stylefactor/floatmktcap.csv\", index_col=0)\n",
    "FactorRtn = pd.read_pickle('BarraFactorData/FactorReturn.pkl')\n",
    "SpecificRtn = pd.read_pickle('BarraFactorData/SpecificReturn.pkl').dropna(how='all',axis=1).iloc[:, :50]\n",
    "MarketValue = pd.read_csv('stylefactor/floatmktcap.csv', index_col=0).iloc[:, :50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0W0Sd4_KhkfH"
   },
   "outputs": [],
   "source": [
    "# 只需要跑一次，保留因子暴露，每天暴露存一个文件\n",
    "beta = pd.read_csv(\"BarraFactorData/Beta.csv\", index_col=0).iloc[:, :50]\n",
    "bp = pd.read_csv(\"BarraFactorData/BookToPrice.csv\", index_col=0).iloc[:, :50]\n",
    "ey = pd.read_csv(\"BarraFactorData/EarningsYield.csv\", index_col=0).iloc[:, :50]\n",
    "growth = pd.read_csv(\"BarraFactorData/Growth.csv\", index_col=0).iloc[:, :50]\n",
    "leverage = pd.read_csv(\"BarraFactorData/Leverage.csv\", index_col=0).iloc[:, :50]\n",
    "liq = pd.read_csv(\"BarraFactorData/Liquidity.csv\", index_col=0).iloc[:, :50]\n",
    "mom = pd.read_csv(\"BarraFactorData/Momentum.csv\", index_col=0).iloc[:, :50]\n",
    "nlsize = pd.read_csv(\"BarraFactorData/NonlinearSize.csv\", index_col=0).iloc[:, :50]\n",
    "resvol = pd.read_csv(\"BarraFactorData/ResidualVolatility.csv\", index_col=0).iloc[:, :50]\n",
    "size = pd.read_csv(\"BarraFactorData/Size.csv\", index_col=0).iloc[:, :50]\n",
    "indus = pd.read_pickle(\"BarraFactorData/Industry.pkl\")\n",
    "\n",
    "# for i in range(len(beta)):\n",
    "#     print(i, beta.iloc[i].name)\n",
    "#     indus_dummies = pd.get_dummies(indus.iloc[i])\n",
    "#     if 'Real Estate' not in indus_dummies:\n",
    "#         print(\"刚开始没有房地产这个行业，加一列0\")\n",
    "#         indus_dummies['Real Estate'] = 0\n",
    "\n",
    "#     date_df = pd.concat([beta.iloc[i], bp.iloc[i],ey.iloc[i], growth.iloc[i],\n",
    "#                          leverage.iloc[i], liq.iloc[i], mom.iloc[i],\n",
    "#                          nlsize.iloc[i], resvol.iloc[i], size.iloc[i],\n",
    "#                          # 注意这里有个坑，行业刚开始是31个后面变成了32个。需要调整\n",
    "#                          indus_dummies], axis=1)\n",
    "#     date_df.columns = ['Beta', 'BookToPrice', 'EarningsYield', 'Growth', 'Leverage',\n",
    "#                        'Liquidity', 'Momentum', 'NonlinearSize', 'ResidualVolatility', 'Size'] + indus_dummies.columns.tolist()\n",
    "#     date_df.insert(0, 'Market', 1)\n",
    "#     date_df = date_df[FactorRtn.columns]\n",
    "#     date_df.to_pickle(\"fac_data/fac_data\" + str(beta.iloc[i].name) + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indus = pd.read_pickle(\"BarraFactorData/Industry.pkl\").iloc[:, :50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfxfyf6mhkfI"
   },
   "source": [
    "数据预处理 参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "CY14W6OphkfM"
   },
   "outputs": [],
   "source": [
    "MarketValue.index = MarketValue.index.astype(str)\n",
    "FactorsValid = FactorRtn.columns.to_list()\n",
    "TradeDates = FactorRtn.index.intersection(SpecificRtn.index).to_list()\n",
    "\n",
    "FactorRtn = FactorRtn.loc[TradeDates,:] # (T, K)\n",
    "SpecificRtn = SpecificRtn.loc[TradeDates,:] # (T, N)\n",
    "MarketValue = MarketValue.loc[TradeDates,:] # (T, N)\n",
    "\n",
    "T = FactorRtn.shape[0] # 历史样本数\n",
    "K = FactorRtn.shape[1] # 风险因子数\n",
    "\n",
    "## 设定计算参数\n",
    "K_inds = 32\n",
    "length = 243\n",
    "lag_F_Corr = 2\n",
    "tau_F_Corr = 480\n",
    "lag_F_Vol = 5\n",
    "# 周度调整为20，月度为81和42\n",
    "tau_F_Vol = 40\n",
    "tau_F_VRA = 20\n",
    "\n",
    "q_S_BS = 0.1 # 特异收益率-贝叶斯收缩 参数\n",
    "lag_S_Vol = 5\n",
    "# 周度调整为20，月度为80和40\n",
    "tau_S_Vol = 40\n",
    "tau_S_VRA = 20\n",
    "\n",
    "FR = FactorRtn.values\n",
    "\n",
    "BF_F_all=[]\n",
    "BF_S_all=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmkCYGMXhkfN"
   },
   "source": [
    "需要用到的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "WTsz-6LihkfO"
   },
   "outputs": [],
   "source": [
    "def uniformize(x):\n",
    "    return x / np.nansum(x)\n",
    "\n",
    "def normalize(x, weight, std_weight=None):\n",
    "    mean = np.nansum(x * uniformize(weight))\n",
    "    if std_weight is not None:\n",
    "        std = np.nanmean((x-mean)**2 * std_weight)\n",
    "    else:\n",
    "        std = np.nanstd(x)\n",
    "    return (x - mean) / std\n",
    "\n",
    "def winsorize(x):\n",
    "    '''确保x是经过标准化的数据'''\n",
    "    s_pos = max(0,min(1,0.5/(max(x)-3)))\n",
    "    s_neg = max(0,min(1,0.5/(-min(x)-3)))\n",
    "    x[x > 3] = 3*(1 - s_pos) + x[x > 3] * s_pos\n",
    "    x[x < -3] = -3*(1 - s_neg) + x[x < -3] * s_neg\n",
    "    return x\n",
    "\n",
    "def winsorize_percentile(x, per_upper=1, per_lower=0):\n",
    "    upper = np.percentile(x, per_upper*100)\n",
    "    lower = np.percentile(x, per_lower*100)\n",
    "    x[x < lower] = lower\n",
    "    x[x > upper] = upper\n",
    "    return x\n",
    "\n",
    "def orthogonalize(raw, ref, weight=None):\n",
    "    if weight is not None:\n",
    "        ortho = raw - (raw * ref * weight).sum() / (ref**2 * weight).sum() * ref\n",
    "    else:\n",
    "        ortho = raw - np.nansum(raw * ref) / np.nansum(ref**2) * ref\n",
    "    return ortho\n",
    "\n",
    "def fillna_with_regression(x, mask, ref):\n",
    "    x_valid = x[mask,None]\n",
    "    ref = np.vstack([np.ones_like(ref),ref]).T\n",
    "    ref_valid = ref[mask,:]\n",
    "    beta = np.linalg.inv(ref_valid.T @ ref_valid) @ ref_valid.T @ x_valid\n",
    "    x[~mask] = (ref[~mask,:] @ beta).ravel()\n",
    "    return x\n",
    "\n",
    "\n",
    "def data_clean(x, w, fillna_ref=None):\n",
    "    ValidMask = ~(np.isnan(x) | np.isinf(x))\n",
    "    x_valid = x[ValidMask]\n",
    "    w_valid = uniformize(w[ValidMask])\n",
    "    x_valid = normalize(winsorize(normalize(x_valid, w_valid)),w_valid)\n",
    "    x[ValidMask] = x_valid\n",
    "    if fillna_ref is not None:\n",
    "        x = normalize(fillna_with_regression(x, ValidMask, fillna_ref), w)\n",
    "    return x\n",
    "\n",
    "def get_EWMA(length=252, tau=90, norm=1):\n",
    "    lambd = 0.5**(1./tau)\n",
    "    w = np.array([lambd**n for n in range(length)][::-1])\n",
    "    if norm:\n",
    "        return uniformize(w)\n",
    "    else:\n",
    "        return w\n",
    "\n",
    "def v_parabolicfit_scaling(v_raw, amp=1.6, k_start_fitting=15):\n",
    "    '''\n",
    "    1、Put v_raw into parabolic fit, get v_p\n",
    "    2、Scale the fitted value in proportion to their deivation from 1, get v_s\n",
    "    '''\n",
    "    K = len(v_raw)\n",
    "    y = v_raw[k_start_fitting:]\n",
    "    x = np.array(range(k_start_fitting,K))\n",
    "    polyfunc = np.poly1d(np.polyfit(x, y, 2))\n",
    "    v_p = np.array([polyfunc(xi) for xi in range(k_start_fitting)] + list(y))\n",
    "    v_s = amp * (v_p - 1) + 1\n",
    "    return v_s\n",
    "\n",
    "def getNeweyWestCov(rtn_data, q, tau, length, n_this, is_NW=1):\n",
    "    '''\n",
    "    Input:\n",
    "        rtn_data            The factor / specific return\n",
    "        q                   Num of periods used to calculate Newey-West Adjusted Covariance\n",
    "        tau                 The length of Halflife Decay\n",
    "        length              How many frames are used to calculate the covariance matrix\n",
    "                            The length must be greater than K (the No. of factors)\n",
    "        n_this              'n_this-length' to 'n_this' frames are used to get the covariance\n",
    "        n_forward           The period ahead to calculate bias B; standard deviation of r_ahead / risk_predicted\n",
    "        is_NW               whether use Newey-West Method\n",
    "\n",
    "     n_this-length           n_this                 n_this+n_forward\n",
    "          |---------------------|-------------------------|\n",
    "                   F                       Bias\n",
    "\n",
    "    Output:\n",
    "        F           if is_NW==True: Newey-West Seriel Correlation Adjustment Covariance;\n",
    "                    elif: Simple Covariance.\n",
    "    '''\n",
    "    q=2\n",
    "    tau=90\n",
    "    if n_this < length:\n",
    "        print('ERROR: n_this should not be less than length')\n",
    "        return\n",
    "    elif n_this > rtn_data.shape[0]:\n",
    "        print('ERROR: n_this + n_forward should not be greater than No. of total frames')\n",
    "        return\n",
    "    f_raw = rtn_data[n_this-length:n_this,:]\n",
    "\n",
    "    # calculate Newey-West covariance\n",
    "    if is_NW:\n",
    "        # 半衰期加权所用权重，wighted average of factors\n",
    "        w = get_EWMA(length, tau, 1)\n",
    "        ewma = np.dot(w[::-1], f_raw)\n",
    "        f = (f_raw - ewma).T\n",
    "        Delta_0 = f @ np.diag(w) @ f.T\n",
    "        # Calculate the cov matrix\n",
    "        F = Delta_0\n",
    "        for d in range(1, q+1):\n",
    "            # 计算滞后d期的协方差（非对称）\n",
    "            Delta_i = f[:,:-d] @ np.diag(w[d:]) @ f[:,d:].T / np.sum(w[d:])\n",
    "            # 乘以Bartlett权重系数\n",
    "            F += ( (1 - d/(q+1.)) * (Delta_i + Delta_i.T) )\n",
    "    else:\n",
    "        F = np.cov(f_raw.T)\n",
    "    # 日频协方差月频化,21 for monthly\n",
    "    F = 21. * F\n",
    "    # decomp of NW covariance, Prepare for Eigenfactor Risk Adjustment\n",
    "    D_diag, U = linalg.eigh(F) # D  in fact the diagonal element of matrix D, a.k.a EigenVar\n",
    "    # r = (data.iloc[n_start:n_start+n_forward,:]+1).cumprod().iloc[-1,:]-1\n",
    "    # R_eigen = np.dot(U.T,r)\n",
    "\n",
    "    # check if U and D_diag is the right eigh-decomposition of F\n",
    "    if not np.allclose(F,  U @ np.diag(D_diag) @ U.T ):\n",
    "        print('ERROR in eigh')\n",
    "        return\n",
    "\n",
    "    return F, U, D_diag\n",
    "\n",
    "def getNeweyWestVol(rtn_data, q, tau, length, n_this, is_NW=1):\n",
    "    '''\n",
    "    单变量版本的Newey-West调整\n",
    "    rtn_data 此处为pd.Seires格式 便于使用apply方法计算。\n",
    "    '''\n",
    "    f_raw = rtn_data.iloc[n_this-length:n_this]\n",
    "    if f_raw.dropna().size < length//4:\n",
    "        ### 数据过少，直接给出波动率为0\n",
    "        Vol = 0\n",
    "        return Vol\n",
    "    f_raw = f_raw.dropna().values\n",
    "    length = f_raw.size\n",
    "    if is_NW:\n",
    "        # 半衰期加权所用权重，wighted average of factors\n",
    "        w = get_EWMA(length, tau, 1)\n",
    "        ewma = np.dot(w[::-1], f_raw)\n",
    "        f = f_raw - ewma\n",
    "        Var_0 = (f**2 * w).sum()\n",
    "        # Calculate the cov matrix\n",
    "        Var = Var_0\n",
    "        for d in range(1, q+1):\n",
    "            # 计算滞后d期的协方差（非对称）\n",
    "            Var_i = (f[:-d] * f[d:] * w[d:]).sum() / np.sum(w[d:])\n",
    "            # 乘以Bartlett权重系数\n",
    "            Var += ( (1 - d/(q+1.)) * (Var_i * 2) )\n",
    "        Vol = np.sqrt(Var)\n",
    "    else:\n",
    "        Vol = np.std(f_raw)\n",
    "    return Vol\n",
    "\n",
    "def getEigenfactorRiskAdjustCov(F_0, U_0, D_diag, tau, length, M=100, is_NW=1): # M should be 10^4 as in report\n",
    "    '''\n",
    "    Input:\n",
    "        F_0         Un-adjusted factor covariance matrix\n",
    "        U_0         KxK rotation matrix whose columns are given by the eigenvectors of F_0.\n",
    "                    U_0[j,k] gives the weight of pure factor j in eigenfactor k .\n",
    "        D_diag      the diagonal element of D, which is the predicted variances of the eigenfactors\n",
    "        M           the total number of simulations\n",
    "        is_NW       whether use Newey-West Method\n",
    "    Output:\n",
    "        F_true      Predicted Factor Covariance After Eigenfactor Risk Adjustment\n",
    "    '''\n",
    "    K = F_0.shape[0]\n",
    "    simulated_volatility_biases = np.zeros((M, K))\n",
    "    for i in range(M):\n",
    "        # if i%50==0:print(i)\n",
    "        b_m = np.array([np.random.normal(0, std, length) for std in np.sqrt(D_diag)]) # generate simulated factor returns\n",
    "        f_m = (U_0 @ b_m).T\n",
    "        F_m, U_m, D_diag_m = getNeweyWestCov(f_m, q=2, tau=tau, length=length, n_this=length, is_NW=is_NW)\n",
    "        D_diag_m_true = np.diagonal(U_m.T @ F_0 @ (U_m))\n",
    "        simulated_volatility_biases[i,:] = D_diag_m_true / D_diag_m\n",
    "    # Simulated Volatility Biases\n",
    "    v_raw = np.sqrt(simulated_volatility_biases.mean(axis=0))\n",
    "    # After Parabolic fit & Scaling\n",
    "    v_s = v_parabolicfit_scaling(v_raw, amp=1.6, k_start_fitting=16)\n",
    "    D_true = np.diag(v_s**2 * D_diag)\n",
    "    F_true = U_0 @ D_true @ U_0.T\n",
    "    return F_true\n",
    "    \n",
    "\n",
    "def getVolatilityRegimeAdjustment(rtn_data, Vol, tau, length, n_this, BF_all):\n",
    "    '''\n",
    "    Input:\n",
    "        rtn_data        The factor / specific return\n",
    "        Vol             The volatility of factor / specific return estimated on day n_this\n",
    "        tau             Halflife parameter of VRA\n",
    "        length          The length of factor_data used to calculate the volatility multiplier\n",
    "        n_this\n",
    "        BF_all          Store each day's Factor Cross-Sectional Bias statistic B^F_t\n",
    "    Output:\n",
    "        Vol_VRA         the Volatility Regime Adjustment forecast of Factor Volatility\n",
    "        BF_all          the same\n",
    "    '''\n",
    "    tau=42\n",
    "    w = get_EWMA(length, tau, 1)\n",
    "    #Vol=np.diag(Vol) # to solve the dimension bug on b\n",
    "    isValidVol = (Vol != 0)\n",
    "    ## b is the cross-section standardized factor return\n",
    "    ## BF is the bias statistic for realized standard deviation of b\n",
    "    #b = rtn_data[n_this,:][isValidVol] / Vol[isValidVol]\n",
    "    b = rtn_data[n_this,:][isValidVol]/ Vol[isValidVol]\n",
    "    BF = np.sqrt(np.mean(b @ b))\n",
    "    BF_all.append(BF)\n",
    "    if n_this > (length+243): # deal with NaN in too early data\n",
    "        # lambda_F is volatility multiplier \n",
    "        lambda_F = np.sqrt( np.power(BF_all[-length:],2) @ w )\n",
    "        Vol_VRA = lambda_F**2 * Vol # lambda_F可以直接平方吗？a number?\n",
    "    else:\n",
    "        Vol_VRA = np.array([np.NaN] * len(Vol))\n",
    "    \n",
    "    return Vol_VRA, BF_all\n",
    "\n",
    "def getSVDAdjustment(cov):\n",
    "    if np.isnan(Cov_F).all():\n",
    "        return None\n",
    "    else:\n",
    "        U,s,V = linalg.svd(cov)\n",
    "        s[s <= 0] = min(s[s > 0]) / 10\n",
    "        cov_adj = U @ np.diag(s) @ V\n",
    "        return cov_adj\n",
    "\n",
    "def getBlendWeight(Rtn_seires, length, n_this):\n",
    "    f_raw = Rtn_seires.iloc[n_this-length:n_this].values\n",
    "    sigma_u = (np.percentile(f_raw, 75) - np.percentile(f_raw, 25)) / 1.35\n",
    "    f_raw[f_raw < -10*sigma_u] = -10*sigma_u\n",
    "    f_raw[f_raw > 10*sigma_u] = 10*sigma_u\n",
    "    sigma_eq = np.std(f_raw)\n",
    "    Z_u = np.abs((sigma_eq-sigma_u)/sigma_u)\n",
    "    gamma = min(1, max(0, np.exp(1-Z_u)))\n",
    "    return gamma\n",
    "\n",
    "def getBlendedSpecificVol(FactorExposure, Gamma, Vol_TS):\n",
    "    mask_gamma = (Gamma == 1)\n",
    "    y = np.log(Vol_TS[mask_gamma])\n",
    "    x = FactorExposure[mask_gamma,:]\n",
    "    b = linalg.inv(x.T @ x) @ x.T @ y\n",
    "    Vol_STR_0 = np.exp(FactorExposure @ b)\n",
    "    Vol_hat = Gamma * Vol_TS + (1 - Gamma) * np.mean(Vol_TS / Vol_STR_0) * Vol_STR_0\n",
    "    return Vol_hat\n",
    "\n",
    "def getBayesianShrinkageResult(Vol, MktCap, n_decile, q):\n",
    "    '''\n",
    "    Input:\n",
    "        Vol             Volaitlity of specific return of each stock\n",
    "        MktCap          Market Value of each stock\n",
    "        n_decile        number of group the stocks will be split into by market value\n",
    "        q               Shrinkage Parameter\n",
    "    Output:\n",
    "        Vol_BS          Volatility after Bayesian Shrinkage\n",
    "    '''\n",
    "    isValidVol = (Vol != 0)\n",
    "    Vol_BS = np.zeros(Vol.size)\n",
    "    Vol = Vol[isValidVol]\n",
    "    MktCap = MktCap[isValidVol]\n",
    "    IdxRankedGrouped = np.array_split(np.argsort(MktCap), n_decile) # 从小到大排列的市值对应序号\n",
    "    # Get the Shrinkage Target of each decile group\n",
    "    VolTarget_G = np.array([np.average(Vol[idxs], weights=uniformize(MktCap[idxs])) for idxs in IdxRankedGrouped])\n",
    "    # Get the standard deviation of specific risk forecasts of each decile group\n",
    "    VolDelta_G = np.array([((Vol[idxs] - vt)**2).sum() / len(idxs) for vt, idxs in zip(VolTarget_G, IdxRankedGrouped)])\n",
    "    # Broadcast group values to all valid vol\n",
    "    VolTarget = np.zeros(Vol.size)\n",
    "    VolDelta = np.zeros(Vol.size)\n",
    "    for i_group, idxs in enumerate(IdxRankedGrouped):\n",
    "        VolTarget[idxs] = VolTarget_G[i_group]\n",
    "        VolDelta[idxs] = VolDelta_G[i_group]\n",
    "    # v is the shrinkage intensity\n",
    "    v = 1 / (VolDelta / q * np.abs(Vol - VolTarget) + 1)\n",
    "    Vol_BS_Valid = v * VolTarget + (1-v) * Vol\n",
    "    Vol_BS[isValidVol] = Vol_BS_Valid\n",
    "    return Vol_BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f17f70d289d4b7988d98d6f39226538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Vol_S_df = pd.DataFrame(index=TradeDates, columns=SpecificRtn.columns, dtype=float)\n",
    "# 滚动计算，跑起来特别慢，从03-23跑了一整天\n",
    "for idx_date in tqdm(range(length,T)):\n",
    "    date = TradeDates[idx_date]\n",
    "    StocksValid = SpecificRtn.loc[date,:].dropna().index.intersection(MarketValue.loc[date,:].dropna().index).to_list()\n",
    "    # print(idx_date, date)\n",
    "    FactorExposure = pd.read_pickle(\"fac_data/fac_data\" + str(date) + \".pkl\")\n",
    "    StocksValid = [i for i in StocksValid if i in FactorExposure.index]\n",
    "    \n",
    "    FactorExposure = FactorExposure.loc[StocksValid,FactorsValid[:-K_inds]].values\n",
    "    SR = SpecificRtn.loc[:,StocksValid].values\n",
    "    thisMV = MarketValue.loc[date,StocksValid].values\n",
    "\n",
    "\n",
    "    # 计算因子收益率的相关系数矩阵\n",
    "    Cov_F_NW_toCorr, U_toCorr, D_diag_toCorr = getNeweyWestCov(FR, q=lag_F_Corr, tau=tau_F_Corr, length=length, n_this=idx_date, is_NW=1)\n",
    "    Cov_F = getEigenfactorRiskAdjustCov(Cov_F_NW_toCorr, U_toCorr, D_diag_toCorr, tau=tau_F_Corr, length=length, M=500, is_NW=0)\n",
    "    Cov_F_tilde = getSVDAdjustment(Cov_F) # 正定化\n",
    "    \n",
    "    # 计算特异收益率的波动率\n",
    "    SpecificRtnValid = SpecificRtn.loc[:,StocksValid]\n",
    "    Vol_S_NW = SpecificRtnValid.apply(lambda x:getNeweyWestVol(x, q=lag_S_Vol, tau=tau_S_Vol, length=length, n_this=idx_date, is_NW=1),axis=0).values\n",
    "    Gamma = SpecificRtnValid.apply(lambda x:getBlendWeight(x, length=length, n_this=idx_date), axis=0).values\n",
    "    Vol_S_Blend = getBlendedSpecificVol(FactorExposure, Gamma, Vol_S_NW)\n",
    "    Vol_S_BS = getBayesianShrinkageResult(Vol_S_Blend, thisMV, n_decile=10, q=q_S_BS)\n",
    "    Vol_S_VRA, BF_S_all = getVolatilityRegimeAdjustment(SR, Vol_S_BS, tau=tau_S_VRA, length=length, n_this=idx_date, BF_all=BF_S_all)\n",
    "    # 记录数据\n",
    "    if Cov_F_tilde is not None:\n",
    "        Vol_S_df.loc[date, StocksValid] = Vol_S_VRA\n",
    "        pd.DataFrame(Cov_F_tilde, index=FactorsValid, columns=FactorsValid).to_pickle(f'FactorCov&SpecificVol/FactorCov_{date}.pkl')\n",
    "        #\n",
    "\n",
    "Vol_S_df = Vol_S_df.dropna(how='all',axis=0)\n",
    "Vol_S_df.to_pickle('FactorCov&SpecificVol/SpecificVol.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWKsSLsVhkfY"
   },
   "source": [
    "现在F,Δ都有了。进一步加工搞成股票协方差矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "oo1UPYY-hkfY"
   },
   "outputs": [],
   "source": [
    "F = pd.read_pickle(\"FactorCov&SpecificVol/FactorCov_20040114.pkl\")\n",
    "Delta = pd.read_pickle(\"FactorCov&SpecificVol/SpecificVol.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20050117\n"
     ]
    }
   ],
   "source": [
    "datei = Delta.index[0]\n",
    "print(datei)\n",
    "F = pd.read_pickle(\"FactorCov&SpecificVol/FactorCov_\" + str(datei) + \".pkl\")\n",
    "X = pd.read_pickle(\"fac_data/fac_data\" + str(datei) + \".pkl\")\n",
    "X = X.loc[:,F.index]\n",
    "common_stocks = [i for i in Delta.columns if i in X.index]\n",
    "\n",
    "# 股票保持一致\n",
    "X = X.loc[common_stocks]\n",
    "Delta_i = Delta[common_stocks]\n",
    "\n",
    "# delta取指定这一天的, 然后变成一个对角矩阵\n",
    "Delta_i = Delta_i.loc[str(datei)]\n",
    "Delta_diag = pd.DataFrame(np.diag(Delta_i))\n",
    "Delta_diag.columns = X.index \n",
    "Delta_diag.index = X.index \n",
    "\n",
    "CovStocks = X @ F @ X.T + Delta_diag\n",
    "# CovStocks.to_parquet(\"StockCov/StockCov_\" + str(datei) + \".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3o5txtjDhkfc",
    "outputId": "3b2afa9b-13b1-46ca-e5e1-7fa92e512a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170203\n",
      "20170210\n",
      "20170217\n",
      "20170224\n",
      "20170303\n",
      "20170310\n",
      "20170317\n",
      "20170324\n",
      "20170331\n",
      "20170407\n",
      "20170414\n",
      "20170421\n",
      "20170428\n",
      "20170505\n",
      "20170512\n",
      "20170519\n",
      "20170526\n",
      "20170602\n",
      "20170609\n",
      "20170616\n",
      "20170623\n",
      "20170630\n",
      "20170707\n",
      "20170714\n",
      "20170721\n",
      "20170728\n",
      "20170804\n",
      "20170811\n",
      "20170818\n",
      "20170825\n",
      "20170901\n",
      "20170908\n",
      "20170915\n",
      "20170922\n",
      "20170929\n",
      "20171013\n",
      "20171020\n",
      "20171027\n",
      "20171103\n",
      "20171110\n",
      "20171117\n",
      "20171124\n",
      "20171201\n",
      "20171208\n",
      "20171215\n",
      "20171222\n",
      "20171229\n",
      "20180105\n",
      "20180112\n",
      "20180119\n",
      "20180126\n",
      "20180202\n",
      "20180209\n",
      "20180214\n",
      "20180223\n",
      "20180302\n",
      "20180309\n",
      "20180316\n",
      "20180323\n",
      "20180330\n",
      "20180404\n",
      "20180413\n",
      "20180420\n",
      "20180427\n",
      "20180504\n",
      "20180511\n",
      "20180518\n",
      "20180525\n",
      "20180601\n",
      "20180608\n",
      "20180615\n",
      "20180622\n",
      "20180629\n",
      "20180706\n",
      "20180713\n",
      "20180720\n",
      "20180727\n",
      "20180803\n",
      "20180810\n",
      "20180817\n",
      "20180824\n",
      "20180831\n",
      "20180907\n",
      "20180914\n",
      "20180921\n",
      "20180928\n",
      "20181012\n",
      "20181019\n",
      "20181026\n",
      "20181102\n",
      "20181109\n",
      "20181116\n",
      "20181123\n",
      "20181130\n",
      "20181207\n",
      "20181214\n",
      "20181221\n",
      "20181228\n",
      "20190104\n",
      "20190111\n",
      "20190118\n",
      "20190125\n",
      "20190201\n",
      "20190215\n",
      "20190222\n",
      "20190301\n",
      "20190308\n",
      "20190315\n",
      "20190322\n",
      "20190329\n",
      "20190404\n",
      "20190412\n",
      "20190419\n",
      "20190426\n",
      "20190430\n",
      "20190510\n",
      "20190517\n",
      "20190524\n",
      "20190531\n",
      "20190606\n",
      "20190614\n",
      "20190621\n",
      "20190628\n",
      "20190705\n",
      "20190712\n",
      "20190719\n",
      "20190726\n",
      "20190802\n",
      "20190809\n",
      "20190816\n",
      "20190823\n",
      "20190830\n",
      "20190906\n",
      "20190912\n",
      "20190920\n",
      "20190927\n",
      "20190930\n",
      "20191011\n",
      "20191018\n",
      "20191025\n",
      "20191101\n",
      "20191108\n",
      "20191115\n",
      "20191122\n",
      "20191129\n",
      "20191206\n",
      "20191213\n",
      "20191220\n",
      "20191227\n",
      "20200103\n",
      "20200110\n",
      "20200117\n",
      "20200123\n",
      "20200207\n",
      "20200214\n",
      "20200221\n",
      "20200228\n",
      "20200306\n",
      "20200313\n",
      "20200320\n",
      "20200327\n",
      "20200403\n",
      "20200410\n",
      "20200417\n",
      "20200424\n",
      "20200430\n",
      "20200508\n",
      "20200515\n",
      "20200522\n",
      "20200529\n",
      "20200605\n",
      "20200612\n",
      "20200619\n",
      "20200624\n",
      "20200703\n",
      "20200710\n",
      "20200717\n",
      "20200724\n",
      "20200731\n",
      "20200807\n",
      "20200814\n",
      "20200821\n",
      "20200828\n",
      "20200904\n",
      "20200911\n",
      "20200918\n",
      "20200925\n",
      "20200930\n",
      "20201009\n",
      "20201016\n",
      "20201023\n",
      "20201030\n",
      "20201106\n",
      "20201113\n",
      "20201120\n",
      "20201127\n",
      "20201204\n",
      "20201211\n",
      "20201218\n",
      "20201225\n",
      "20201231\n",
      "20210108\n",
      "20210115\n",
      "20210122\n",
      "20210129\n",
      "20210205\n",
      "20210210\n",
      "20210219\n",
      "20210226\n",
      "20210305\n",
      "20210312\n",
      "20210319\n",
      "20210326\n",
      "20210402\n",
      "20210409\n",
      "20210416\n",
      "20210423\n",
      "20210430\n",
      "20210507\n",
      "20210514\n",
      "20210521\n",
      "20210528\n",
      "20210604\n",
      "20210611\n",
      "20210618\n",
      "20210625\n",
      "20210702\n",
      "20210709\n",
      "20210716\n",
      "20210723\n",
      "20210730\n",
      "20210806\n",
      "20210813\n",
      "20210820\n",
      "20210827\n",
      "20210903\n",
      "20210910\n",
      "20210917\n",
      "20210924\n",
      "20210930\n",
      "20211008\n",
      "20211015\n",
      "20211022\n",
      "20211029\n",
      "20211105\n",
      "20211112\n",
      "20211119\n",
      "20211126\n",
      "20211203\n",
      "20211210\n",
      "20211217\n",
      "20211224\n",
      "20211231\n",
      "20220107\n",
      "20220114\n",
      "20220121\n",
      "20220128\n",
      "20220211\n",
      "20220218\n",
      "20220225\n",
      "20220304\n",
      "20220311\n",
      "20220318\n",
      "20220325\n",
      "20220401\n",
      "20220408\n",
      "20220415\n",
      "20220422\n",
      "20220429\n",
      "20220506\n",
      "20220513\n",
      "20220520\n",
      "20220527\n",
      "20220602\n",
      "20220610\n",
      "20220617\n",
      "20220624\n",
      "20220701\n",
      "20220708\n",
      "20220715\n",
      "20220722\n",
      "20220729\n",
      "20220805\n",
      "20220812\n",
      "20220819\n",
      "20220826\n",
      "20220902\n",
      "20220909\n",
      "20220916\n",
      "20220923\n",
      "20220930\n",
      "20221014\n",
      "20221021\n",
      "20221028\n",
      "20221104\n",
      "20221111\n",
      "20221118\n",
      "20221125\n",
      "20221202\n",
      "20221209\n",
      "20221216\n",
      "20221223\n",
      "20221230\n",
      "20230106\n",
      "20230113\n",
      "20230120\n",
      "20230203\n",
      "20230210\n",
      "20230217\n",
      "20230224\n",
      "20230303\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FactorCov&SpecificVol/FactorCov_20230303.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m datei \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(weeklastday[i])\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(datei)\n\u001b[1;32m----> 7\u001b[0m F \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_pickle(\u001b[39m\"\u001b[39;49m\u001b[39mFactorCov&SpecificVol/FactorCov_\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(datei) \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m X \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m\"\u001b[39m\u001b[39mfac_data/fac_data\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(datei) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mloc[:,F\u001b[39m.\u001b[39mindex]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\pickle.py:190\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[39mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[39m4    4    9\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m excs_to_catch \u001b[39m=\u001b[39m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mModuleNotFoundError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m)\n\u001b[1;32m--> 190\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    191\u001b[0m     filepath_or_buffer,\n\u001b[0;32m    192\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    193\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    194\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    195\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    196\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    197\u001b[0m \n\u001b[0;32m    198\u001b[0m     \u001b[39m# 1) try standard library Pickle\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[39m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[39m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m         \u001b[39m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[0;32m    204\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[0;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[0;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FactorCov&SpecificVol/FactorCov_20230303.pkl'"
     ]
    }
   ],
   "source": [
    "# Delta = pd.read_pickle(\"FactorCov&SpecificVol/SpecificVol.pkl\")\n",
    "# Delta\n",
    "\n",
    "# for i in range(860,len(weeklastday)):\n",
    "#     datei = str(weeklastday[i])\n",
    "#     print(datei)\n",
    "#     F = pd.read_pickle(\"FactorCov&SpecificVol/FactorCov_\" + str(datei) + \".pkl\")\n",
    "\n",
    "#     X = pd.read_pickle(\"fac_data/fac_data\" + str(datei) + \".pkl\")\n",
    "#     X = X.loc[:,F.index]\n",
    "\n",
    "#     common_stocks = [i for i in Delta.columns if i in X.index]\n",
    "\n",
    "#     # 股票保持一致\n",
    "#     X = X.loc[common_stocks]\n",
    "#     Delta_i = Delta[common_stocks]\n",
    "\n",
    "#     # delta取指定这一天的, 然后变成一个对角矩阵\n",
    "#     Delta_i = Delta_i.loc[str(datei)]\n",
    "#     Delta_diag = pd.DataFrame(np.diag(Delta_i))\n",
    "#     Delta_diag.columns = X.index\n",
    "#     Delta_diag.index = X.index\n",
    "\n",
    "#     CovStocks = X @ F @ X.T + Delta_diag\n",
    "#     CovStocks.to_parquet(\"StockCov_WeekLast/StockCov_\" + str(datei) + \".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vxlBwB9hkfc"
   },
   "outputs": [],
   "source": [
    "weeklastday = pd.read_csv(\"weeklastday.csv\", index_col=0)\n",
    "weeklastday = weeklastday['0'].tolist()\n",
    "\n",
    "weekfirstday = pd.read_csv(\"weekfirstday.csv\", index_col=0)\n",
    "weekfirstday = weekfirstday['0'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BYXaM9_Hhkfd",
    "outputId": "5f998506-76f5-441a-f2ca-96e0155fb95e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1157"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weeklastday.index(20221125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkOiIIKMhkfe",
    "outputId": "e6496566-976e-45c9-ee2a-57c37fa8db56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20191101\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'StockCov/StockCov_20191101.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m,\u001b[39mlen\u001b[39m(weeklastday)):\n\u001b[0;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(weeklastday[i])\n\u001b[1;32m----> 5\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopyfile(\u001b[39m\"\u001b[39;49m\u001b[39mStockCov/StockCov_\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(weeklastday[i]) \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mStockCov_WeekLast/StockCov_\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(weeklastday[i]) \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Python\\lib\\shutil.py:264\u001b[0m, in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    262\u001b[0m     os\u001b[39m.\u001b[39msymlink(os\u001b[39m.\u001b[39mreadlink(src), dst)\n\u001b[0;32m    263\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(src, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fsrc, \u001b[39mopen\u001b[39m(dst, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fdst:\n\u001b[0;32m    265\u001b[0m         \u001b[39m# macOS\u001b[39;00m\n\u001b[0;32m    266\u001b[0m         \u001b[39mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[0;32m    267\u001b[0m             \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'StockCov/StockCov_20191101.parquet'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "for i in range(1000,len(weeklastday)):\n",
    "\n",
    "    print(weeklastday[i])\n",
    "    shutil.copyfile(\"StockCov/StockCov_\" + str(weeklastday[i]) + \".parquet\", \"StockCov_WeekLast/StockCov_\" + str(weeklastday[i]) + \".parquet\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
